{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403e70cc",
   "metadata": {},
   "source": [
    "# 1、层和块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c82387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0575,  0.0646, -0.1531, -0.2502,  0.2134, -0.2487,  0.1119, -0.2793,\n",
       "         -0.0771,  0.2374],\n",
       "        [ 0.0821, -0.1458, -0.1899, -0.3212,  0.2784, -0.1832,  0.1434, -0.3080,\n",
       "         -0.2397,  0.1752]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F      #定义了一些有用的函数\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20,256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256,10)\n",
    "                   )\n",
    "X = torch.rand(2 ,20)         #2为批处理大小\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e6004",
   "metadata": {},
   "source": [
    "nn.Sequential 定义一种特殊的Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f88851",
   "metadata": {},
   "source": [
    "## 1.1、自定义MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "723c9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):                               #继承nn.Module\n",
    "    def __init__(self):                             #定义组件\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)             #定义输入为20输出为256的线性层\n",
    "        self.out = nn.Linear(256,10)\n",
    "    def forward(self ,X):                           #定义前向传播\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c38b827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1075,  0.1499,  0.0019, -0.1983,  0.1055,  0.0090,  0.0848,  0.0717,\n",
       "          0.2854, -0.1110],\n",
       "        [ 0.1165,  0.0994, -0.0794, -0.2184,  0.1116, -0.0013,  0.0785, -0.0218,\n",
       "          0.3297, -0.1320]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3f531",
   "metadata": {},
   "source": [
    "## 1.2、顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3867f7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0464, -0.1645,  0.0924, -0.0242,  0.1961,  0.2435,  0.0517, -0.0803,\n",
       "         -0.0844, -0.1829],\n",
       "        [ 0.1450, -0.2823,  0.0479, -0.0597,  0.1657,  0.1154,  0.0692, -0.0372,\n",
       "         -0.1532, -0.2183]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequentail(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for block in args:                            #\n",
    "            self._modules[block] = block               # 特殊的容器，按序的字典\n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "net = MySequentail(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b7220",
   "metadata": {},
   "source": [
    "## 1.3、灵活构造函数，在正向传播中执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ca7b90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1398, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):           #在init和forward中可以做大量的自定义计算\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20,20),requires_grad=False)    #随机weight不参与训练\n",
    "        self.linear = nn.Linear(20,20)\n",
    "    def forward(self ,X):\n",
    "        X =  self.linear(X)                           #先做完linear\n",
    "        X = F.relu(torch.mm(X,self.rand_weight)+1)    #和随机weights做乘法\n",
    "        X = self.linear(X)                            #在调用linear\n",
    "        while X.abs().sum()>1:                       #对X绝对值求和如果大于一，除二\n",
    "            X/=2\n",
    "        return X.sum()\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac3fac",
   "metadata": {},
   "source": [
    "## 1.4、嵌套使用Sequential或自定义方法自由组合搭配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07e64e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0030, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20,64),nn.ReLU(),\n",
    "                                 nn.Linear(64,32),nn.ReLU()\n",
    "                                )\n",
    "        self.linear = nn.Linear(32,16)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimra = nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP())\n",
    "chimra(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b1720",
   "metadata": {},
   "source": [
    "## 2、参数管理\n",
    "    首先关注单隐藏层的多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c287d0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3017],\n",
       "        [-0.3544]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "X = torch.rand(size=(2,4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40f801",
   "metadata": {},
   "source": [
    "### 2.1 参数访问\n",
    "    将每一层中的权重拿出来"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a5e27",
   "metadata": {},
   "source": [
    "sequential可以看做为python 的list ； net[2]相当于拿到nn.Linear(8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d75a74b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.3533,  0.0262, -0.1229, -0.0872,  0.3234, -0.3202, -0.2343,  0.2017]])), ('bias', tensor([-0.1021]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c264274",
   "metadata": {},
   "source": [
    "### 2.2、访问具体的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90c09c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.1021], requires_grad=True)\n",
      "tensor([-0.1021])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)     #访问数据，其中还有梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be2c9051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef57cf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "### 2.3、一次性访问所有参数\n",
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()])\n",
    "print(*[(name,param.shape) for name,param in net.named_parameters()])                #* 是对list进行拆解，访问liet中每一组数据， 解包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7571527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1021])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data          #通过某一层的名字直接访问其中数值   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c32e444",
   "metadata": {},
   "source": [
    "### 2.3、嵌套网络中收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae7fdc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2302],\n",
       "        [-0.2303]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),nn.ReLU(), nn.Linear(8,4),nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}',block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(),nn.Linear(4,1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe8e8f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2bed72",
   "metadata": {},
   "source": [
    "## 3、内置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cbc7cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0060, 0.0060, 0.0031, 0.0024]), tensor(0.))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):                                  #定义正太分布\n",
    "    if type(m)==nn.Linear:                           #如果是线性层做均值为0，方差为0.01的初始化\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)    #有_ 的函数表示是替换函数，原地操作 ，将其中的weights替换掉\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f94cab6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec110792",
   "metadata": {},
   "source": [
    "### 3.1、对某些块使用不同的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6ee7c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0405, -0.6049, -0.5710,  0.0473])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight,42)\n",
    "\n",
    "net[0].apply(xavier)                    #对第一个层使用Xavier初始化\n",
    "net[2].apply(init_42)                   #对最后一个层使用init_42的初始化\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ce539",
   "metadata": {},
   "source": [
    "### 3.2 自定义初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6be405d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.5630, -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -8.5134, -6.0404]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\n",
    "            \"Init\",\n",
    "            *[(name,param.shape) for name , param in m.named_parameters()][0]\n",
    "        )\n",
    "        nn.init.uniform_(m.weight,-10,10)\n",
    "        m.weight.data *= m.weight.data.abs() >=5         # 当前权重数据乘以(自己绝对值中，保留大于等于5的值，其余为0）\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c624b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 暴力直接修改\n",
    "net[0].weight.data[:] +=1\n",
    "net[0].weight.data[0,0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725737e",
   "metadata": {},
   "source": [
    "### 3.3、参数绑定\n",
    "    小应用：在一些层之间share一些param，连个输入数据流进来如何共享权重\n",
    "    先构造shared层，构造出来时参数就生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57787c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared = nn.Linear(8,8)\n",
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),shared,nn.ReLU(),shared,nn.ReLU(),nn.Linear(8,1))\n",
    "\n",
    "net(X)\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0,0] == 100\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1180a15",
   "metadata": {},
   "source": [
    "## 4、自定义层\n",
    "    构造一个没有任何参数的自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f3565d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self ,X):\n",
    "        return X - X.mean()\n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d9e17",
   "metadata": {},
   "source": [
    "### 4.1 将自己构造的层放入sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26e4dfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8626e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(8,128),CenteredLayer())\n",
    "\n",
    "Y = net(torch.rand(4,8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f6b12",
   "metadata": {},
   "source": [
    "### 4.2 自定义带有参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dead7da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0804, -0.5038, -1.3182],\n",
       "        [-0.2190,  0.1457,  1.5468],\n",
       "        [ 0.0219, -0.7152, -0.6811],\n",
       "        [-0.8235,  2.0442,  1.8378],\n",
       "        [-1.4888, -0.1597,  0.9653]], requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self,in_units,units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units,units))  #两参数， 随机初始化(输入大小，输出大小)矩阵\n",
    "        self.bias = nn.Parameter(torch.randn(units,))               #randn 正态分布  ；加逗号是创建列向量\n",
    "        \n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X,self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "\n",
    "dense = MyLinear(5,3)\n",
    "dense.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b2f5b1",
   "metadata": {},
   "source": [
    "###  4.3 自定层直接执行正向传播计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50ec3cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1066, 1.1407, 0.4707],\n",
       "        [0.1582, 2.8575, 1.7879]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense(torch.rand(2,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d92bd",
   "metadata": {},
   "source": [
    "###  4.4 自定层构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b857a3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.2691],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64,8),MyLinear(8,1))\n",
    "net(torch.rand(2,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791fe12",
   "metadata": {},
   "source": [
    "## 5、读写文件\n",
    "    加载和保存张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0974b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04 数据操作 + 数据预处理【动手学深度学习v2】.ipynb  \u001b[0m\u001b[01;34md2l-zh\u001b[0m/\r\n",
      "16 PyTorch 神经网络基础【动手学深度学习v2】.ipynb   V10多层感知机.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b6c73fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "torch.save(x,'x-file')\n",
    "x2 = torch.load(\"x-file\")\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a933dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04 数据操作 + 数据预处理【动手学深度学习v2】.ipynb  \u001b[0m\u001b[01;34md2l-zh\u001b[0m/              x-file\r\n",
      "16 PyTorch 神经网络基础【动手学深度学习v2】.ipynb   V10多层感知机.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aed495",
   "metadata": {},
   "source": [
    "### 5.1、存一个张量列表，然后将它们读回内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f5935897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x,y],'x-file')\n",
    "x2,y2 = torch.load(\"x-file\")\n",
    "(x2,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1742b7",
   "metadata": {},
   "source": [
    "### 5.2、写入并读取字符串映射的张量字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f92055b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x':x , 'y':y}\n",
    "torch.save(mydict,'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b40ec0e",
   "metadata": {},
   "source": [
    "### 5.3、加载和保存模型参数  \n",
    "    pytorch不存储计算图，只存储权重参数，torchScript存储计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "18876fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.output = nn.Linear(256,10)\n",
    "    def forward(self,x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "    \n",
    "net = MLP()\n",
    "X = torch.randn(size=(2,20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8418d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型参数存储一个mlp.params\n",
    "torch.save(net.state_dict(),'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e34e98",
   "metadata": {},
   "source": [
    "实例化原始多层感知机模型，直接读取文件中存储的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6905d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load(\"mlp.params\"))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04d359e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)                   #clone一个网络，输入和同样之前的随机X，拿到Y 进行对比\n",
    "Y_clone == Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch18",
   "language": "python",
   "name": "torch18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
